"Item type","Authors","Editors","Title","Journal","Publication year","Volume","Issue","Pages","Publisher","Proceedings title","Date published","ISSN","URLs","DOI","Abstract","Keywords","Series"
"Journal Article","Ganesh AH,Xu B","","A review of reinforcement learning based energy management systems for electrified powertrains: Progress, challenge, and potential solution","Renewable and Sustainable Energy Reviews","2022","154","","111833","","","2022","1364-0321","https://www.sciencedirect.com/science/article/pii/S136403212101100X;http://dx.doi.org/10.1016/j.rser.2021.111833","10.1016/j.rser.2021.111833","The impact of internal combustion engine-powered automobiles on climate change due to emissions and the depletion of fossil fuels has contributed to the progress of electrified powertrains. Energy management strategies (EMS) have shown huge impact on the energy efficiency of the electrified powertrains. In recent years, Reinforcement Learning (RL) based algorithms have been intensely investigated to develop EMS for electrified powertrain with specific depth in hybrid electric vehicles (HEV), battery electric vehicles (BEV) and fuel cell vehicles (FCV) and the research in this area is still acelerating. However, a comprehensive review of RL-based EMS is lacking in literature. This article reviews the recent penetration of RL based EMS like Q-learning, Deep Q Learning, deep deterministic policy gradient in the electrified powertrains domain. Extensive importance is given to the classification of the literature based on powertrain architecture, RL algorithm, and the different features and operation mechanisms of relevant algorithms are highlighted. The use of connected and autonomous vehicles and relevant communication technology to develop RL-based systems have also been discussed. More importantly, the challenges with regards to existing research in the field of RL-based EMS and the potential solutions with scope for future research are also discussed.","Electrified powertrain, Reinforcement learning, Energy management strategies, Deep reinforcement learning, Connected & autonomous vehicles",""
"Journal Article","Moos J,Hansel K,Abdulsamad H,Stark S,Clever D,Peters J","","Robust Reinforcement Learning: A Review of Foundations and Recent Advances","Machine Learning and Knowledge Extraction","2022","4","1","276-315","","","2022","2504-4990","https://www.mdpi.com/2504-4990/4/1/13;http://dx.doi.org/10.3390/make4010013","10.3390/make4010013","Reinforcement learning (RL) has become a highly successful framework for learning in Markov decision processes (MDP). Due to the adoption of RL in realistic and complex environments, solution robustness becomes an increasingly important aspect of RL deployment. Nevertheless, current RL algorithms struggle with robustness to uncertainty, disturbances, or structural changes in the environment. We survey the literature on robust approaches to reinforcement learning and categorize these methods in four different ways: (i) Transition robust designs account for uncertainties in the system dynamics by manipulating the transition probabilities between states; (ii) Disturbance robust designs leverage external forces to model uncertainty in the system behavior; (iii) Action robust designs redirect transitions of the system by corrupting an agent’s output; (iv) Observation robust designs exploit or distort the perceived system state of the policy. Each of these robust designs alters a different aspect of the MDP. Additionally, we address the connection of robustness to the risk-based and entropy-regularized RL formulations. The resulting survey covers all fundamental concepts underlying the approaches to robust reinforcement learning and their recent advances.","",""
"Journal Article","Palminteri S,Lebreton M","","The computational roots of positivity and confirmation biases in reinforcement learning","Trends in Cognitive Sciences","2022","26","7","607-621","","","2022","1364-6613","https://www.sciencedirect.com/science/article/pii/S1364661322000894;http://dx.doi.org/10.1016/j.tics.2022.04.005","10.1016/j.tics.2022.04.005","Humans do not integrate new information objectively: outcomes carrying a positive affective value and evidence confirming one’s own prior belief are overweighed. Until recently, theoretical and empirical accounts of the positivity and confirmation biases assumed them to be specific to ‘high-level’ belief updates. We present evidence against this account. Learning rates in reinforcement learning (RL) tasks, estimated across different contexts and species, generally present the same characteristic asymmetry, suggesting that belief and value updating processes share key computational principles and distortions. This bias generates over-optimistic expectations about the probability of making the right choices and, consequently, generates over-optimistic reward expectations. We discuss the normative and neurobiological roots of these RL biases and their position within the greater picture of behavioral decision-making theories.","confirmation, decision, gain, learning, loss, update",""
"Journal Article","Fu Q,Han Z,Chen J,Lu Y,Wu H,Wang Y","","Applications of reinforcement learning for building energy efficiency control: A review","Journal of Building Engineering","2022","50","","104165","","","2022","2352-7102","https://www.sciencedirect.com/science/article/pii/S2352710222001784;http://dx.doi.org/10.1016/j.jobe.2022.104165","10.1016/j.jobe.2022.104165","The wide variety of smart devices equipped in modern intelligent buildings and the increasing comfort requirements of occupants for the environment make the control of intelligent buildings important and complex. Reinforcement learning, as a class of control techniques in machine learning, has been explored for its potential in the field of intelligent building control. Reinforcement learning methods applied to intelligent buildings can effectively reduce energy consumption. In this paper, we classify reinforcement learning algorithms and analyze the control problems that each algorithm is suitable for solving. In addition, we review the reinforcement learning methods applied to control and manage buildings, outline the problems and future directions of reinforcement learning applications in intelligent buildings, and give our suggestions for researchers who want to use reinforcement learning methods to solve control problems in this field.","Reinforcement learning, Intelligent buildings, Energy consumption",""
"Journal Article","Ladosz P,Weng L,Kim M,Oh H","","Exploration in deep reinforcement learning: A survey","Information Fusion","2022","85","","1-22","","","2022","1566-2535","https://www.sciencedirect.com/science/article/pii/S1566253522000288;http://dx.doi.org/10.1016/j.inffus.2022.03.003","10.1016/j.inffus.2022.03.003","This paper reviews exploration techniques in deep reinforcement learning. Exploration techniques are of primary importance when solving sparse reward problems. In sparse reward problems, the reward is rare, which means that the agent will not find the reward often by acting randomly. In such a scenario, it is challenging for reinforcement learning to learn rewards and actions association. Thus more sophisticated exploration methods need to be devised. This review provides a comprehensive overview of existing exploration approaches, which are categorised based on the key contributions as: reward novel states, reward diverse behaviours, goal-based methods, probabilistic methods, imitation-based methods, safe exploration and random-based methods. Then, unsolved challenges are discussed to provide valuable future research directions. Finally, the approaches of different categories are compared in terms of complexity, computational effort and overall performance.","Deep reinforcement learning, Exploration, Intrinsic motivation, Sparse reward problems",""
"Journal Article","Matsuo Y,LeCun Y,Sahani M,Precup D,Silver D,Sugiyama M,Uchibe E,Morimoto J","","Deep learning, reinforcement learning, and world models","Neural Networks","2022","152","","267-275","","","2022","0893-6080","https://www.sciencedirect.com/science/article/pii/S0893608022001150;http://dx.doi.org/10.1016/j.neunet.2022.03.037","10.1016/j.neunet.2022.03.037","Deep learning (DL) and reinforcement learning (RL) methods seem to be a part of indispensable factors to achieve human-level or super-human AI systems. On the other hand, both DL and RL have strong connections with our brain functions and with neuroscientific findings. In this review, we summarize talks and discussions in the “Deep Learning and Reinforcement Learning” session of the symposium, International Symposium on Artificial Intelligence and Brain Science. In this session, we discussed whether we can achieve comprehensive understanding of human intelligence based on the recent advances of deep learning and reinforcement learning algorithms. Speakers contributed to provide talks about their recent studies that can be key technologies to achieve human-level intelligence.","Deep learning, Reinforcement learning, World models, Machine learning, Artificial intelligence",""
"Journal Article","Stiennon N,Ouyang L,Wu J,Ziegler D,Lowe R,Voss C,Radford A,Amodei D,Christiano PF","Larochelle H,Ranzato M,Hadsell R,Balcan MF,Lin H","Learning to summarize with human feedback","","2020","33","","3008-3021","Curran Associates, Inc.","","2020","","https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf","","As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about—summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts [63] and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles [22], producing summaries nearly as good as the human reference without any news-specific fine-tuning.2 We conduct extensive analyses to understand our human feedback dataset and fine-tuned models.3 We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.","",""
"Journal Article","Gao L,Schulman J,Hilton J","","Scaling Laws for Reward Model Overoptimization","arXiv preprint arXiv:2210. 10760","2022","","","","","","2022","","","","In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed ""gold-standard"" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.","",""
"Journal Article","Goldstein JA,Sastry G,Musser M,DiResta R,Gentzel M,Sedova K","","Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations","arXiv preprint arXiv:2301. 04246","2023","","","","","","2023","","","","Generative language models have improved drastically, and can now produce realistic text outputs that are difficult to distinguish from human-written content. For malicious actors, these language models bring the promise of automating the creation of convincing and misleading text for use in influence operations. This report assesses how language models might change influence operations in the future, and what steps can be taken to mitigate this threat. We lay out possible changes to the actors, behaviors, and content of online influence operations, and provide a framework for stages of the language model-to-influence operations pipeline that mitigations could target (model construction, model access, content dissemination, and belief formation). While no reasonable mitigation can be expected to fully prevent the threat of AI-enabled influence operations, a combination of multiple mitigations may make an important difference.","",""
"Journal Article","Le N,Rathour VS,Yamazaki K,Luu K,Savvides M","","Deep reinforcement learning in computer vision: a comprehensive survey","Artificial Intelligence Review","2022","55","4","2733-2819","","","2022-04","","","","Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i) landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision.","",""
"Journal Article","Wu D,Lei Y,He M,Zhang C,Ji L","","Deep Reinforcement Learning-Based Path Control and Optimization for Unmanned Ships","Wireless Communications and Mobile Computing","2022","2022","","7135043","Hindawi","","2022-05","","","","Unmanned ship navigates on the water in an autonomous or semiautonomous way, which can be widely used in maritime transportation, intelligence collection, maritime training and testing, reconnaissance, and evidence collection. In this paper, we use deep reinforcement learning to solve the optimization problem in the path planning and management of unmanned ships. Specifically, we take the waiting time (phase and duration) at the corner of the path as the optimization goal to minimize the total travel time of unmanned ships passing through the path. We propose a new reward function, which considers the environment and control delay of unmanned ships at the same time, which can reduce the coordination time between unmanned ships at the same time. In the simulation experiment, through the quantitative and qualitative results of deep reinforcement learning of unmanned ship navigation and path angle waiting, the effectiveness of our solution is verified.","",""
"Journal Article","Gronauer S,Diepold K","","Multi-agent deep reinforcement learning: a survey","Artificial Intelligence Review","2022","55","2","895-943","","","2022-02","","","","The advances in reinforcement learning have recorded sublime success in various domains. Although the multi-agent domain has been overshadowed by its single-agent counterpart during this progress, multi-agent reinforcement learning gains rapid traction, and the latest accomplishments address problems with real-world complexity. This article provides an overview of the current developments in the field of multi-agent deep reinforcement learning. We focus primarily on literature from recent years that combines deep reinforcement learning methods with a multi-agent scenario. To survey the works that constitute the contemporary landscape, the main contents are divided into three parts. First, we analyze the structure of training schemes that are applied to train multiple agents. Second, we consider the emergent patterns of agent behavior in cooperative, competitive and mixed scenarios. Third, we systematically enumerate challenges that exclusively arise in the multi-agent domain and review methods that are leveraged to cope with these challenges. To conclude this survey, we discuss advances, identify trends, and outline possible directions for future work in this research area.","",""
"Conference Paper","Rudin N,Hoeller D,Reist P,Hutter M","Faust A,Hsu D,Neumann G","Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning","","2022","164","","91-100","PMLR","Proceedings of the 5th Conference on Robot Learning","08--11 Nov 2022","","https://proceedings.mlr.press/v164/rudin22a.html","","In this work, we present and study a training set-up that achieves fast policy generation for real-world robotic tasks by using massive parallelism on a single workstation GPU. We analyze and discuss the impact of different training algorithm components in the massively parallel regime on the final policy performance and training times. In addition, we present a novel game-inspired curriculum that is well suited for training with thousands of simulated robots in parallel. We evaluate the approach by training the quadrupedal robot ANYmal to walk on challenging terrain. The parallel approach allows training policies for flat terrain in under four minutes, and in twenty minutes for uneven terrain. This represents a speedup of multiple orders of magnitude compared to previous work. Finally, we transfer the policies to the real robot to validate the approach. We open-source our training code to help accelerate further research in the field of learned legged locomotion: https://leggedrobotics.github.io/legged_gym/.","","Proceedings of Machine Learning Research"
"Journal Article","Eppe M,Gumbsch C,Kerzel M,Nguyen PD,Butz MV,Wermter S","","Intelligent problem-solving as integrated hierarchical reinforcement learning","Nature Machine Intelligence","2022","4","1","11-20","","","2022-01","","","","According to cognitive psychology and related disciplines, the development of complex problem-solving behaviour in biological agents depends on hierarchical cognitive mechanisms. Hierarchical reinforcement learning is a promising computational approach that may eventually yield comparable problem-solving behaviour in artificial agents and robots. However, so far, the problem-solving abilities of many human and non-human animals are clearly superior to those of artificial systems. Here we propose steps to integrate biologically inspired hierarchical mechanisms to enable advanced problem-solving skills in artificial agents. We first review the literature in cognitive psychology to highlight the importance of compositional abstraction and predictive processing. Then we relate the gained insights with contemporary hierarchical reinforcement learning methods. Interestingly, our results suggest that all identified cognitive mechanisms have been implemented individually in isolated computational architectures, raising the question of why there exists no single unifying architecture that integrates them. As our final contribution, we address this question by providing an integrative perspective on the computational challenges to develop such a unifying architecture. We expect our results to guide the development of more sophisticated cognitively inspired hierarchical machine learning architectures.","",""
"Conference Paper","Cheng CA,Xie T,Jiang N,Agarwal A","Chaudhuri K,Jegelka S,Song L,Szepesvari C,Niu G,Sabato S","Adversarially Trained Actor Critic for Offline Reinforcement Learning","","2022","162","","3852-3878","PMLR","Proceedings of the 39th International Conference on Machine Learning","17--23 Jul 2022","","https://proceedings.mlr.press/v162/cheng22b.html","","We propose Adversarially Trained Actor Critic (ATAC), a new model-free algorithm for offline reinforcement learning (RL) under insufficient data coverage, based on the concept of relative pessimism. ATAC is designed as a two-player Stackelberg game framing of offline RL: A policy actor competes against an adversarially trained value critic, who finds data-consistent scenarios where the actor is inferior to the data-collection behavior policy. We prove that, when the actor attains no regret in the two-player game, running ATAC produces a policy that provably 1) outperforms the behavior policy over a wide range of hyperparameters that control the degree of pessimism, and 2) competes with the best policy covered by data with appropriately chosen hyperparameters. Compared with existing works, notably our framework offers both theoretical guarantees for general function approximation and a deep RL implementation scalable to complex environments and large datasets. In the D4RL benchmark, ATAC consistently outperforms state-of-the-art offline RL algorithms on a range of continuous control tasks.","","Proceedings of Machine Learning Research"
"Journal Article","Li Q,Peng Z,Feng L,Zhang Q,Xue Z,Zhou B","","MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning","IEEE Transactions on Pattern Analysis and Machine Intelligence","2022","","","1-14","","","2022","","http://dx.doi.org/10.1109/TPAMI.2022.3190471","10.1109/TPAMI.2022.3190471","Driving safely requires multiple capabilities from human and intelligent agents, such as the generalizability to unseen environments, the safety awareness of the surrounding traffic, and the decision-making in complex multi-agent settings. Despite the great success of Reinforcement Learning (RL), most of the RL research works investigate each capability separately due to the lack of integrated environments. In this work, we develop a new driving simulation platform called MetaDrive to support the research of generalizable reinforcement learning algorithms for machine autonomy. MetaDrive is highly compositional, which can generate an infinite number of diverse driving scenarios from both the procedural generation and the real data import ing. Based on MetaDrive, we construct a variety of RL tasks and baselines in both single-agent and multi-agent settings, including benchmarking generalizability across unseen scenes, safe exploration, and learning multi-agent traffic. The generalization experiments conducted on both procedurally generated scenarios and real-world scenarios show that increasing the diversity and the size of the training set leads to the improvement of the RL agent’s generalizability. We further evaluate various safe reinforcement learning and multi-agent reinforcement learning algorithms in MetaDrive environments and provide the benchmarks. Source code, documentation, and demo video are available at https://metadriverse.github.io/metadrive.","",""
"Journal Article","Hayes CF,Rădulescu R,Bargiacchi E,Källström J,Macfarlane M,Reymond M,Verstraeten T,Zintgraf LM,Dazeley R,Heintz F,Howley E,Irissappane AA,Mannion P,Nowé A,Ramos G,Restelli M,Vamplew P,Roijers DM","","A practical guide to multi-objective reinforcement learning and planning","Autonomous Agents and Multi-Agent Systems","2022","36","1","26","","","2022-04","","","","Real-world sequential decision-making tasks are generally complex, requiring trade-offs between multiple, often conflicting, objectives. Despite this, the majority of research in reinforcement learning and decision-theoretic planning either assumes only a single objective, or that multiple objectives can be adequately handled via a simple linear combination. Such approaches may oversimplify the underlying problem and hence produce suboptimal results. This paper serves as a guide to the application of multi-objective methods to difficult problems, and is aimed at researchers who are already familiar with single-objective reinforcement learning and planning methods who wish to adopt a multi-objective perspective on their research, as well as practitioners who encounter multi-objective decision problems in practice. It identifies the factors that may influence the nature of the desired solution, and illustrates by example how these influence the design of multi-objective decision-making systems for complex problems.","",""
